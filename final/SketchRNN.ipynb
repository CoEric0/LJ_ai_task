{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.mode = 'train' # train or test\n",
    "\n",
    "\n",
    "        # 数据的部分特征\n",
    "        self.data_lable = 'pig'\n",
    "        self.data_path = './sketch_datas/'\n",
    "        self.data_labels_list = ['ambulance','apple','bear','bicycle','bird','bus','cat','foot','owl','pig']\n",
    "        self.data_max = 1500 # 每个类别的最大数据量,节省训练时间\n",
    "        # self.full_length = 256 # 一条数据的完整长度\n",
    "        self.N_max = 0 # 一条数据的最大长度，可修改\n",
    "        self.lengths = [] # 一条数据的实际长度，可修改\n",
    "\n",
    "\n",
    "        # 编码器的参数设置\n",
    "        self.encoder_dim = 256 # encoder的输出维度\n",
    "        self.z_dim = 128 # 中间向量Z的维度\n",
    "\n",
    "        # 解码器的参数设置\n",
    "        self.decoder_dim = 512 # decoder的输出维度\n",
    "        self.M = 20 # GMM的分布数量\n",
    "\n",
    "        # 训练参数\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = 'mps'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.grad_clip = 1.0 # 梯度裁剪\n",
    "        self.batch_size = 16\n",
    "        self.lr = 0.001\n",
    "        self.epoch = 10\n",
    "        self.W_kl = 0.5 # kl损失的权重\n",
    "        self.temperature = 0.4 # gumbel softmax的温度参数\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 数据\n",
    "读入数据，并对数据进行预处理\n",
    "数据集由配置文件决定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from re import split\n",
    "\n",
    "def get_max(arr):\n",
    "    return arr.shape[1]\n",
    "    \n",
    "\n",
    "# 按照7:1:2划分数据集\n",
    "def split_data(arr, ratios):\n",
    "    # 计算分割点\n",
    "    total_length = arr.shape[0]\n",
    "    splits = [int(total_length * ratio / sum(ratios)) for ratio in ratios]\n",
    "    \n",
    "    splits[-1] = total_length - sum(splits[:-1])\n",
    "\n",
    "    # 按照分割点分割数组\n",
    "    split_arrays = []\n",
    "    start = 0\n",
    "    for split in splits:\n",
    "        split_arrays.append(arr[start:start + split])\n",
    "        start += split\n",
    "\n",
    "    return split_arrays\n",
    "\n",
    "# 按照batch_size随机划分数据集\n",
    "def split_in_batch(arr, config):\n",
    "    # 划分batch，不足batchsize的直接丢弃\n",
    "    batch_num = arr.shape[0] // config.batch_size \n",
    "    max_length = batch_num * config.batch_size # 最长有效数据长度\n",
    "\n",
    "    # 打乱数据\n",
    "    shuffled_arr = arr[:max_length].copy()\n",
    "    np.random.shuffle(shuffled_arr)\n",
    "\n",
    "    split_arrays = np.array(np.split(shuffled_arr, batch_num))\n",
    "\n",
    "    return split_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pig dataset is loaded\n",
      "N_max: 151 ,data_type: float32 , batch_size: 16\n",
      "train_data: (75, 16, 151, 5)\n",
      "val_data: (9, 16, 151, 5)\n",
      "test_data: (9, 16, 151, 5)\n"
     ]
    }
   ],
   "source": [
    "config = Config() # 初始化相关配置\n",
    "\n",
    "\n",
    "f_path = config.data_path + config.data_lable + '.npy'\n",
    "f = np.load(f_path)[:config.data_max] # 最多只用1000条数据训练\n",
    "config.N_max = get_max(f) # 获取最大长度，数据已补全\n",
    "train_data, val_data, test_data = split_data(f,[8,1,1])\n",
    "\n",
    "train_data = split_in_batch(train_data, config).astype(np.float32)\n",
    "val_data = split_in_batch(val_data, config).astype(np.float32)\n",
    "test_data = split_in_batch(test_data, config).astype(np.float32)\n",
    "\n",
    "\n",
    "# length列表\n",
    "config.lengths = [config.N_max] * config.batch_size \n",
    "\n",
    "print(config.data_lable, 'dataset is loaded')\n",
    "print('N_max:',config.N_max,',data_type:', train_data.dtype,', batch_size:',config.batch_size)\n",
    "print('train_data:',train_data.shape)\n",
    "print('val_data:',val_data.shape)\n",
    "print('test_data:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# 2 \n",
    "\n",
    "class encoderRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(encoderRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(5, config.encoder_dim, bidirectional=True) # 双向lstm\n",
    "        self.fc_mu = nn.Linear(config.encoder_dim*2, config.z_dim)\n",
    "        self.fc_sigma = nn.Linear(config.encoder_dim*2, config.z_dim)\n",
    "\n",
    "\n",
    "    def forward(self, config, input):\n",
    "        h0 = torch.zeros(2, config.batch_size, config.encoder_dim).to(config.device)\n",
    "        c0 = torch.zeros(2, config.batch_size, config.encoder_dim).to(config.device)\n",
    "        hidden_cell = (h0, c0)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(input, hidden_cell)\n",
    "\n",
    "        # 完成编码，调整数据形状\n",
    "        # (2, batch_size, hidden_size) -> (batch_size, 2*hidden_size)\n",
    "        hidden_forward, hidden_backward = torch.split(hidden, 1, 0)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)], 1)\n",
    "        \n",
    "        # mu部分\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "\n",
    "        # sigma部分\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat / 2.0)\n",
    "\n",
    "        # gaussian noise\n",
    "        N = torch.randn_like(mu)\n",
    "\n",
    "        Z = mu + sigma * N\n",
    "\n",
    "\n",
    "        return Z, mu, sigma_hat\n",
    "\n",
    "# 2\n",
    "class decoderRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(decoderRNN, self).__init__()\n",
    "        self.N_max = config.N_max\n",
    "        \n",
    "        self.lstm = nn.LSTM(config.z_dim + 5, config.decoder_dim)\n",
    "        self.fc_hc = nn.Linear(config.z_dim, 2 * config.decoder_dim)\n",
    "\n",
    "        # 6M+3\n",
    "        self.fc_output = nn.Linear(config.decoder_dim, 6 * config.M + 3)\n",
    "        # self.fc_q = nn.Linear(config.decoder_dim, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, config, input, z):\n",
    "        h0, c0 = torch.split(torch.tanh(self.fc_hc(z)), config.decoder_dim, 1)\n",
    "        hidden_cell = ((h0.unsqueeze(0).contiguous()), c0.unsqueeze(0).contiguous()) # [batch_size, lstm_size] -> [1, batch_size, lstm_size]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(input, hidden_cell)\n",
    "\n",
    "        y = self.fc_output(output.view(-1, config.decoder_dim))\n",
    "\n",
    "        # 分割\n",
    "        params = torch.split(y, 6, dim=1)\n",
    "        params_mixture = torch.stack(params[:-1])\n",
    "        params_pen = params[-1]\n",
    "\n",
    "        pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy = torch.split(params_mixture, 1, dim=2)\n",
    "\n",
    "        # pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy = torch.split(self.fc_xy(output), config.M, 2) # 6M\n",
    "        # q = self.fc_q(output) # 3\n",
    "\n",
    "        # 后处理 \n",
    "        # 这些变量的维度？\n",
    "        pi = nn.functional.softmax(pi.transpose(0, 1).squeeze(), dim=-1).view(self.N_max+1, -1, config.M)\n",
    "        sigma_x = torch.exp(sigma_x.transpose(0, 1).squeeze()).view(self.N_max+1, -1, config.M)\n",
    "        sigma_y = torch.exp(sigma_y.transpose(0, 1).squeeze()).view(self.N_max+1, -1, config.M)\n",
    "        rho_xy = torch.tanh(rho_xy.transpose(0, 1).squeeze()).view(self.N_max+1, -1, config.M)\n",
    "        mu_x = mu_x.transpose(0, 1).squeeze().contiguous().view(self.N_max+1, -1, config.M)\n",
    "        mu_y = mu_y.transpose(0, 1).squeeze().contiguous().view(self.N_max+1, -1, config.M)\n",
    "        q = nn.functional.softmax(params_pen, dim=-1).view(self.N_max+1, -1, 3)\n",
    "\n",
    "        return pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4 训练设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1\n",
    "class model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(model, self).__init__()\n",
    "        self.encoder = encoderRNN(config).to(config.device)\n",
    "        self.decoder = decoderRNN(config).to(config.device)\n",
    "\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=config.lr)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=config.lr)\n",
    "\n",
    "    \n",
    "    # KL Loss\n",
    "    def KL_loss(self, mu, sigma, config):\n",
    "        return -0.5 * torch.sum(1 + sigma - mu.pow(2) - torch.exp(sigma)) / float(config.z_dim * config.batch_size)\n",
    "\n",
    "    # Reconstruction Loss\n",
    "    def RC_loss(self, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, mask, dx, dy, p, config):\n",
    "        # pdf\n",
    "        z_x = ((dx - mu_x) / sigma_x) ** 2\n",
    "        z_y = ((dy - mu_y) / sigma_y) ** 2\n",
    "        z_xy = (dx - mu_x) * (dy - mu_y) / (sigma_x * sigma_y)\n",
    "        z = z_x + z_y - 2 * rho_xy * z_xy\n",
    "        exp = torch.exp(-z / (2 * (1 - rho_xy ** 2)))\n",
    "        norm = 2 * np.pi * sigma_x * sigma_y * torch.sqrt(1 - rho_xy ** 2)\n",
    "        pdf = torch.nan_to_num(exp / norm, 0.0)\n",
    "\n",
    "        loss_LS = -torch.sum(mask * torch.log(1e-5 + torch.sum(pi * pdf, dim=2))) / float(config.N_max * config.batch_size) # XY\n",
    "        loss_LP = -torch.sum(p * torch.log(q)) / float(config.N_max * config.batch_size) # PPP\n",
    "\n",
    "        return loss_LS + loss_LP\n",
    "\n",
    "    # 处理数据标签（补全，记录有效数据的长度mask机制，分割“笔画、笔状态数据”）\n",
    "    def make_target(self, config, batch):\n",
    "        eos = torch.stack([torch.Tensor([0, 0, 0, 0, 1])] * batch.size()[1]).to(config.device).unsqueeze(0)\n",
    "        batch = torch.cat([batch, eos], 0)\n",
    "        # 记录有效数据的长度mask机制\n",
    "        mask = torch.zeros(config.N_max + 1, batch.shape[1]).to(config.device) \n",
    "        for idx, length in enumerate(config.lengths):\n",
    "            mask[:length, idx] = 1 # 真实有效数据才能进行后续计算\n",
    "\n",
    "        # 分割“笔画、笔状态数据”\n",
    "        dx = torch.stack([batch.data[:, :, 0]] * config.M, 2)\n",
    "        dy = torch.stack([batch.data[:, :, 1]] * config.M, 2)\n",
    "        p1 = batch.data[:, :, 2]\n",
    "        p2 = batch.data[:, :, 3]\n",
    "        p3 = batch.data[:, :, 4]\n",
    "        p = torch.stack([p1, p2, p3], dim=2)\n",
    "\n",
    "        return mask, dx, dy, p\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # 储存模型（解码编码分开保存）\n",
    "    def save(self, config, model):\n",
    "        # save encoder and decoder\n",
    "        torch.save(model.encoder.state_dict(), f\"trained_SRNN_{config.data_lable}_encoder.pth\")\n",
    "        torch.save(model.decoder.state_dict(), f\"trained_SRNN_{config.data_lable}_decoder.pth\")\n",
    "\n",
    "\n",
    "    def train(self, config, train_data, val_data):\n",
    "        train_loss_epoch_history = []\n",
    "        train_LKL_epoch_history = []\n",
    "        train_LR_epoch_history = []\n",
    "\n",
    "        val_loss_epoch_history = []\n",
    "        val_LKL_epoch_history = []\n",
    "        val_LR_epoch_history = []\n",
    "\n",
    "\n",
    "        best_val_loss = 100000000 # 初始化一个较大的值\n",
    "        best_model = None\n",
    "        \n",
    "        train_data = torch.from_numpy(train_data).to(config.device)\n",
    "        val_data = torch.from_numpy(val_data).to(config.device)\n",
    "\n",
    "\n",
    "        for epoch in range(config.epoch):\n",
    "            print(f'Start training, now epoch {epoch}',end = '\\t')\n",
    "            train_loss_history = []\n",
    "            train_LKL_history = []\n",
    "            train_LR_history = []\n",
    "\n",
    "            val_loss_history = []\n",
    "            val_LKL_history = []\n",
    "            val_LR_history = []\n",
    "\n",
    "\n",
    "\n",
    "            # 训练阶段\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "\n",
    "            for train_batch in tqdm(train_data): \n",
    "                # encoder\n",
    "                train_batch = train_batch.permute(1,0,2)\n",
    "                train_z, train_mu, train_sigma = self.encoder(config, train_batch)\n",
    "\n",
    "                # decoder\n",
    "                # S0数据准备\n",
    "                s0 = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * config.batch_size).to(config.device).unsqueeze(0) # [1, batch_size, 5]\n",
    "\n",
    "                train_batch_init = torch.cat([s0, train_batch], dim = 0) # 引入s0的数据,[N_max+1, batch_size, 5]\n",
    "                train_z_stack = torch.stack([train_z] * (config.N_max + 1)) # [N_max+1, batch_size, z_dim]\n",
    "                train_inputs = torch.cat([train_batch_init, train_z_stack], dim=2) # [N_max+1, batch_size, 5+z_dim]\n",
    "\n",
    "                train_pi, train_mu_x, train_mu_y, train_sigma_x, train_sigma_y, train_rho_xy, train_q, _, _ = self.decoder(config, train_inputs, train_z)\n",
    "\n",
    "                # 计算损失\n",
    "                train_mask, train_dx, train_dy, train_p = self.make_target(config, train_batch) # y_true数据\n",
    "             \n",
    "                rc_loss = self.RC_loss(train_pi, train_mu_x, train_mu_y, train_sigma_x, train_sigma_y, train_rho_xy, train_q, train_mask, train_dx, train_dy, train_p, config)\n",
    "                kl_loss = self.KL_loss(train_mu, train_sigma, config)\n",
    "                train_loss = rc_loss + config.W_kl *kl_loss \n",
    "\n",
    "                # 记录训练损失\n",
    "                train_loss_history.append(train_loss.item())\n",
    "                train_LKL_history.append(kl_loss.item())\n",
    "                train_LR_history.append(rc_loss.item())\n",
    "\n",
    "\n",
    "                # 反向传播\n",
    "                self.encoder_optimizer.zero_grad()\n",
    "                self.decoder_optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                \n",
    "                clip_grad_norm_(self.encoder.parameters(), config.grad_clip)\n",
    "                clip_grad_norm_(self.decoder.parameters(), config.grad_clip)\n",
    "                self.encoder_optimizer.step()\n",
    "                self.decoder_optimizer.step()\n",
    "\n",
    "\n",
    "                # 验证val阶段\n",
    "                self.encoder.eval()\n",
    "                self.decoder.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for val_batch in val_data:\n",
    "                        # encoder\n",
    "                        val_batch = val_batch.permute(1,0,2)\n",
    "                        val_z, val_mu, val_sigma = self.encoder(config, val_batch)\n",
    "\n",
    "                        # decoder\n",
    "\n",
    "                        val_s0 = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * config.batch_size).to(config.device).unsqueeze(0) # [1, batch_size, 5]\n",
    "\n",
    "                        val_batch_init = torch.cat([val_s0, val_batch], dim = 0)\n",
    "                        val_z_stack = torch.stack([val_z] * (config.N_max + 1))\n",
    "                        val_inputs = torch.cat([val_batch_init, val_z_stack], dim=2)\n",
    "\n",
    "                        val_pi, val_mu_x, val_mu_y, val_sigma_x, val_sigma_y, val_rho_xy, val_q, _, _ = self.decoder(config, val_inputs, val_z)\n",
    "\n",
    "                        # 计算损失\n",
    "                        val_mask, val_dx, val_dy, val_p = self.make_target(config, val_batch) # y_true数据\n",
    "\n",
    "                        val_rc_loss = self.RC_loss(val_pi, val_mu_x, val_mu_y, val_sigma_x, val_sigma_y, val_rho_xy, val_q, val_mask, val_dx, val_dy, val_p, config)\n",
    "                        val_kl_loss = self.KL_loss(val_mu, val_sigma, config)\n",
    "                        val_loss = val_rc_loss + config.W_kl * val_kl_loss\n",
    "\n",
    "                        # 记录验证损失\n",
    "                        val_loss_history.append(val_loss.item())\n",
    "                        val_LKL_history.append(val_kl_loss.item())\n",
    "                        val_LR_history.append(val_rc_loss.item())\n",
    "\n",
    "\n",
    "            # 记录每个epoch的损失\n",
    "            train_loss_epoch_history.append(np.mean(train_loss_history).item())\n",
    "            train_LKL_epoch_history.append(np.mean(train_LKL_history).item())\n",
    "            train_LR_epoch_history.append(np.mean(train_LR_history).item())\n",
    "\n",
    "            val_loss_epoch_history.append(np.mean(val_loss_history).item())\n",
    "            val_LKL_epoch_history.append(np.mean(val_LKL_history).item())\n",
    "            val_LR_epoch_history.append(np.mean(val_LR_history).item())\n",
    "\n",
    "\n",
    "            # 保存最优的模型\n",
    "            if best_val_loss > min(val_loss_epoch_history):\n",
    "                best_model = self # 更新\n",
    "                \n",
    "        # end of epoch for\n",
    "\n",
    "        # 保存模型\n",
    "        self.save(config, best_model)\n",
    "\n",
    "        # 展示训练过程\n",
    "        plt.figure()\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_loss_epoch_history, label='train loss')\n",
    "        plt.plot(train_LR_epoch_history, label='train reconstruction loss')\n",
    "        plt.plot(train_LKL_epoch_history, label='train KL_loss')\n",
    "        plt.title(f'Train Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # save val loss graph\n",
    "        plt.figure()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(val_loss_epoch_history, label='val loss')\n",
    "        plt.plot(val_LR_epoch_history, label='val reconstruction loss')\n",
    "        plt.plot(val_LKL_epoch_history, label='val KL_loss')\n",
    "        plt.title(f'Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # 使用测试集数据测试模型（展示生成结果）\n",
    "    def test(self, config, test_data):\n",
    "        pass\n",
    "        \n",
    "\n",
    "\n",
    "        # 最终生成样例(有条件)\n",
    "        # self.generate(config, test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training, now epoch 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:30<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training, now epoch 1\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:29<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training, now epoch 2\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 43/75 [00:16<00:12,  2.59it/s]"
     ]
    }
   ],
   "source": [
    "my_model = model(config)\n",
    "\n",
    "my_model.train(config, train_data, val_data)\n",
    "\n",
    "# my_model.test(config, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 展示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
