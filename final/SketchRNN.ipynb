{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.mode = 'train' # train or test\n",
    "\n",
    "\n",
    "        # 数据的部分特征\n",
    "        self.data_lable = 'pig'\n",
    "        self.data_path = './sketch_datas/'\n",
    "        self.data_labels_list = ['ambulance','apple','bear','bicycle','bird','bus','cat','foot','owl','pig']\n",
    "        # self.full_length = 256 # 一条数据的完整长度\n",
    "        self.N_max = 0 # 一条数据的最大长度，可修改\n",
    "\n",
    "\n",
    "        # 编码器的参数设置\n",
    "        self.encoder_dim = 256 # encoder的输出维度\n",
    "        self.z_dim = 128 # 中间向量Z的维度\n",
    "\n",
    "        # 解码器的参数设置\n",
    "        self.decoder_dim = 512 # decoder的输出维度\n",
    "        self.M = 20 # GMM的分布数量\n",
    "\n",
    "        # 训练参数\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = 'mps'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.batch_size = 16\n",
    "        self.lr = 0.001\n",
    "        self.epoch = 40\n",
    "        self.W_kl = 0.5 # kl损失的权重\n",
    "        self.temperature = 0.4 # gumbel softmax的温度参数\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 数据\n",
    "读入数据，并对数据进行预处理\n",
    "数据集由配置文件决定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from re import split\n",
    "\n",
    "def get_max(arr):\n",
    "    return arr.shape[1]\n",
    "    \n",
    "\n",
    "# 按照7:1:2划分数据集\n",
    "def split_data(arr, ratios):\n",
    "    # 计算分割点\n",
    "    total_length = arr.shape[0]\n",
    "    splits = [int(total_length * ratio / sum(ratios)) for ratio in ratios]\n",
    "    \n",
    "    splits[-1] = total_length - sum(splits[:-1])\n",
    "\n",
    "    # 按照分割点分割数组\n",
    "    split_arrays = []\n",
    "    start = 0\n",
    "    for split in splits:\n",
    "        split_arrays.append(arr[start:start + split])\n",
    "        start += split\n",
    "\n",
    "    return split_arrays\n",
    "\n",
    "# 按照batch_size随机划分数据集\n",
    "def split_in_batch(arr, config):\n",
    "    # 划分batch，不足batchsize的直接丢弃\n",
    "    batch_num = arr.shape[0] // config.batch_size \n",
    "    max_length = batch_num * config.batch_size # 最长有效数据长度\n",
    "\n",
    "    # 打乱数据\n",
    "    shuffled_arr = arr[:max_length].copy()\n",
    "    np.random.shuffle(shuffled_arr)\n",
    "\n",
    "    split_arrays = np.array(np.split(shuffled_arr, batch_num))\n",
    "\n",
    "    return split_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pig dataset is loaded\n",
      "N_max: 151 , batch_size: 16\n",
      "train_data: (3062, 16, 151, 5)\n",
      "val_data: (437, 16, 151, 5)\n",
      "test_data: (875, 16, 151, 5)\n"
     ]
    }
   ],
   "source": [
    "config = Config() # 初始化相关配置\n",
    "\n",
    "\n",
    "f_path = config.data_path + config.data_lable + '.npy'\n",
    "f = np.load(f_path)\n",
    "config.N_max = get_max(f) # 获取最大长度，数据已补全\n",
    "train_data, val_data, test_data = split_data(f,[7,1,2])\n",
    "\n",
    "train_data = split_in_batch(train_data, config)\n",
    "val_data = split_in_batch(val_data, config)\n",
    "test_data = split_in_batch(test_data, config)\n",
    "\n",
    "\n",
    "print(config.data_lable, 'dataset is loaded')\n",
    "print('N_max:',config.N_max,', batch_size:',config.batch_size)\n",
    "print('train_data:',train_data.shape)\n",
    "print('val_data:',val_data.shape)\n",
    "print('test_data:',test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# 2 \n",
    "\n",
    "class encoderRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(encoderRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(5, config.encoder_dim, bidirectional=True) # 双向lstm\n",
    "        self.fc_mu = nn.Linear(config.encoder_dim*2, config.z_dim)\n",
    "        self.fc_sigma = nn.Linear(config.encoder_dim*2, config.z_dim)\n",
    "\n",
    "\n",
    "    def forward(self, input, batchsize, config):\n",
    "        h0 = torch.zeros(2, batchsize, config.encoder_dim).to(config.device)\n",
    "        c0 = torch.zeros(2, batchsize, config.encoder_dim).to(config.device)\n",
    "        hidden_cell = (h0, c0)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(input.float(), hidden_cell)\n",
    "\n",
    "        # 完成编码，调整数据形状\n",
    "        # (2, batch_size, hidden_size) -> (batch_size, 2*hidden_size)\n",
    "        hidden_forward, hidden_backward = torch.split(hidden, 1, 0)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)], 1)\n",
    "        \n",
    "        # mu部分\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "\n",
    "        # sigma部分\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat / 2.0)\n",
    "\n",
    "        # gaussian noise\n",
    "        N = torch.randn_like(mu)\n",
    "\n",
    "        Z = mu + sigma * N\n",
    "\n",
    "\n",
    "        return Z, mu, sigma_hat\n",
    "\n",
    "# 2\n",
    "class decoderRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(decoderRNN, self).__init__()\n",
    "        self.N_max = config.N_max\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(config.z_dim + 5, config.decoder_dim)\n",
    "        self.fc_hc = nn.Linear(config.z_dim, 2 * config.decoder_dim)\n",
    "\n",
    "        # 6M+3\n",
    "        self.fc_output = nn.Linear(config.decoder_dim, 6 * config.M)\n",
    "        self.fc_q = nn.Linear(config.decoder_dim, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, config, input, z):\n",
    "        h0, c0 = torch.split(torch.tanh(self.fc_hc(z)), config.decoder_dim, 1)\n",
    "        hidden_cell = (h0.unsqueeze(0).contiguous, c0.unsqueeze(0).contiguous) # [batch_size, lstm_size] -> [1, batch_size, lstm_size]\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(input, hidden_cell)\n",
    "\n",
    "        # y = self.fc_output(output.view(-1, config.decoder_dim))\n",
    "\n",
    "        pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy = torch.split(self.fc_xy(output), self.M, 2) # 6M\n",
    "        q = self.fc_q(output) # 3\n",
    "\n",
    "        # 后处理 \n",
    "        # 这些变量的维度？\n",
    "        pi = nn.functional.softmax(pi.transpose(0, 1).squeeze(), dim=-1).view(self.N_max+1, -1, config.M)\n",
    "        sigma_x = torch.exp(sigma_x.transpose(0, 1).squeeze()).view(self.N_max+1, -1, config.M)\n",
    "        sigma_y = torch.exp(sigma_y.transpose(0, 1).squeeze()).view(self.N_max+1, -1, config.M)\n",
    "        rho_xy = torch.tanh(rho_xy.transpose(0, 1).squeeze()).view(self.N_max+1, -1, config.M)\n",
    "        mu_x = mu_x.transpose(0, 1).squeeze().contiguous().view(self.N_max+1, -1, config.M)\n",
    "        mu_y = mu_y.transpose(0, 1).squeeze().contiguous().view(self.N_max+1, -1, config.M)\n",
    "\n",
    "        return pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4 训练设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "# 1\n",
    "class model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(model, self).__init__()\n",
    "        self.encoder = encoderRNN(config).to(config.device)\n",
    "        self.decoder = decoderRNN(config).to(config.device)\n",
    "\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=config.learning_rate)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    \n",
    "    # KL Loss\n",
    "    def KL_loss(self, mu, sigma, config):\n",
    "        return -0.5 * torch.sum(1 + sigma - mu.pow(2) - torch.exp(sigma)) / float(config.z_dim * config.batch_size)\n",
    "\n",
    "\n",
    "    # Reconstruction Loss\n",
    "    def RC_loss(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # 储存模型（解码编码分开保存）\n",
    "    def save(self, config, epoch):\n",
    "        pass\n",
    "\n",
    "    # 读取模型（解码编码分开读取）\n",
    "    def load(self, config, epoch):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def train(self, config, train_data, val_data):\n",
    "        train_loss_history = []\n",
    "        train_LKL_history = []\n",
    "        train_LR_history = []\n",
    "        val_loss_history = []\n",
    "        val_LKL_history = []\n",
    "        val_LR_history = []\n",
    "        \n",
    "        train_data = torch.from_numpy(train_data).to(config.device)\n",
    "        val_data = torch.from_numpy(val_data).to(config.device)\n",
    "\n",
    "\n",
    "        for epoch in tqdm(range(config.num_epochs)):\n",
    "            # 训练阶段\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "\n",
    "            for train_batch in train_data: \n",
    "                # encoder\n",
    "                train_z, train_mu, train_sigma = self.encoder(train_batch, config.batch_size)\n",
    "\n",
    "                # decoder\n",
    "                # S0数据准备\n",
    "                s0 = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * config.batch_size).to(config.device).unsqueeze(0) # [1, batch_size, 5]\n",
    "\n",
    "                train_batch_init = torch.cat([s0, train_batch], dim = 0) # 引入s0的数据,[N_max+1, batch_size, 5]\n",
    "                train_z_stack = torch.stack([train_z] * (config.N_max + 1)) # [N_max+1, batch_size, z_dim]\n",
    "                train_inputs = torch.cat([train_batch_init, train_z_stack], dim=2) # [N_max+1, batch_size, 5+z_dim]\n",
    "\n",
    "                train_pi, train_mu_x, train_mu_y, train_sigma_x, train_sigma_y, train_rho_xy, train_q, _, _ = self.decoder(config, train_inputs, train_z)\n",
    "\n",
    "                # 多种损失计算，最后相加\n",
    "                kl_loss = config.W_kl * self.KL_loss(train_mu, train_sigma, config)\n",
    "\n",
    "                make_target() # 处理数据标签（补全，记录有效数据的长度mask机制，分割“笔画、笔状态数据”）\n",
    "\n",
    "\n",
    "                train_loss = kl_loss + r_loss\n",
    "\n",
    "\n",
    "                train_loss_history.append(train_loss.item())\n",
    "\n",
    "\n",
    "                # 反向传播\n",
    "                self.encoder_optimizer.zero_grad()\n",
    "                self.decoder_optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.encoder.parameters(), config.grad_clip)\n",
    "                nn.utils.clip_grad_norm_(self.decoder.parameters(), config.grad_clip)\n",
    "                self.encoder_optimizer.step()\n",
    "                self.decoder_optimizer.step()\n",
    "\n",
    "\n",
    "                # 验证val阶段\n",
    "                if epoch > config.num_epochs/10:\n",
    "\n",
    "                    self.encoder.eval()\n",
    "                    self.decoder.eval()\n",
    "\n",
    "                    make_batch()\n",
    "\n",
    "                    # encode\n",
    "\n",
    "\n",
    "                    # decode\n",
    "                    # S0数据准备\n",
    "\n",
    "\n",
    "                    # 计算损失\n",
    "\n",
    "                    val_loss = kl_loss + r_loss\n",
    "\n",
    "\n",
    "                    val_loss_history.append(val_loss.item()) # 列表，保存val_loss\n",
    "\n",
    "                    # 保存最优的模型\n",
    "                    if val_loss < val_loss_history.max():\n",
    "                        self.save(epoch)\n",
    "\n",
    "\n",
    "    # 使用测试集数据测试模型（展示生成结果）\n",
    "    def test(self, config, test_data):\n",
    "        \n",
    "\n",
    "\n",
    "        # 最终生成样例(有条件)\n",
    "        self.generate(config, test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 展示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
