{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一 复合函数的计算图，计算器雅可比行列式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Computational_Graph:\n",
    "    def __init__(self) -> None:\n",
    "    #初始化x1，x2，x3\n",
    "        self.x1 = 0\n",
    "        self.x2 = 0\n",
    "        self.x3 = 0\n",
    "        #初始化z1，z2，z3，用以存储z1，z2，z3的计算结果\n",
    "        self.z1 = 0\n",
    "        self.z2 = 0\n",
    "        self.z3 = 0\n",
    "        #初始化u1，u2，u3，用以存储u1，u2，u3的计算结果\n",
    "        self.u1 = 0\n",
    "        self.u2 = 0\n",
    "        self.u3 = 0\n",
    "        #初始化v1，v2，v3，用以存储v1，v2，v3的计算结果\n",
    "        self.u1 = 0\n",
    "        self.u2 = 0\n",
    "        self.u3 = 0\n",
    "        #初始化y1，y2，用以存储y1，y2的计算结果\n",
    "        self.y1 = 0\n",
    "        self.y2 = 0 \n",
    "        \n",
    "    def function_z1(self,x1,x2):   \n",
    "        return 2*x1 + x2\n",
    "    def function_z2(self,x1,x3):\n",
    "        return  x1 * 3 * x3   \n",
    "    def function_z3(self,x3):\n",
    "        return  -x3\n",
    "    def function_u1(self,z1):\n",
    "        return  np.sin(self.z1)\n",
    "    def function_u2(self,x3,z2):\n",
    "        return (2 * x3)+z2\n",
    "    def function_u3(self,z1,z3):\n",
    "        return (2 * z1) + z3\n",
    "    def function_v1(self,u1,u3):    \n",
    "        return u1 - u3\n",
    "    def function_v2(self,u2): \n",
    "        return np.sin(-1*u2)      \n",
    "    def function_v3(self,u1,u3):\n",
    "        return u1 * u3\n",
    "    def function_y1(self,v1,v2):\n",
    "        return v1**2 + v2**3\n",
    "    def function_y2(self, v2, v3):\n",
    "        return v2*v3\n",
    "\n",
    "#前向传播\n",
    "    def forward(self,x1,x2,x3):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.x3 = x3\n",
    "\n",
    "        self.z1 = self.function_z1(self.x1, self.x2)\n",
    "        self.z2 = self.function_z2(self.x1, self.x3)\n",
    "        self.z3 = self.function_z3(self.x3)\n",
    "\n",
    "        self.u1 = self.function_u1(self.z1)\n",
    "        self.u2 = self.function_u2(self.x3, self.z2)\n",
    "        self.u3 = self.function_u3(self.z1, self.z3)\n",
    "\n",
    "        self.v1 = self.function_v1(self.u1, self.u3)\n",
    "        self.v2 = self.function_v2(self.u2)\n",
    "        self.v3 = self.function_v3(self.u1, self.u3)\n",
    "\n",
    "        self.y1 = self.function_y1(self.v1, self.v2)\n",
    "        self.y2 = self.function_y2(self.v2, self.v3)\n",
    "\n",
    "        print(f'y1 = {self.y1}, y2 = {self.y2}')\n",
    "\n",
    "\n",
    "\n",
    "#反向传播\n",
    "    def backward(self):\n",
    "        self.y1_to_v1 = 2 * self.v1\n",
    "        self.y1_to_v2 = 3*(self.v2**2)\n",
    "\n",
    "        self.y2_to_v2 = self.v3\n",
    "        self.y2_to_v3 = self.v2\n",
    "\n",
    "        # backward\n",
    "        self.v1_to_u1 = 1\n",
    "        self.v1_to_u3 = -1\n",
    "\n",
    "        self.v2_to_u2 = -1 * np.cos(-1 * self.u2)\n",
    "        \n",
    "        self.v3_to_u1 = self.u3\n",
    "        self.v3_to_u3 = self.u1\n",
    "\n",
    "\n",
    "        # back\n",
    "        self.u1_to_z1 = np.cos(self.z1)\n",
    "\n",
    "        self.u2_to_x3 = 2 \n",
    "        self.u2_to_z2 = 1\n",
    "\n",
    "        self.u3_to_z1 = 2\n",
    "        self.u3_to_z3 = 1\n",
    "\n",
    "\n",
    "        # back\n",
    "        self.z1_to_x1 = 2\n",
    "        self.z1_to_x2 = 1\n",
    "\n",
    "        self.z2_to_x1 = 3*self.x3\n",
    "        self.z2_to_x3 = 3*self.x1\n",
    "\n",
    "        self.z3_to_x3 = -1\n",
    "\n",
    "        # all \n",
    "        self.y1_to_x1 = self.y1_to_v1 * (self.v1_to_u1*(self.u1_to_z1*self.z1_to_x1) + self.v1_to_u3 * self.u3_to_z1*self.z1_to_x1) + self.y1_to_v2*self.v2_to_u2*self.u2_to_z2*self.z2_to_x1\n",
    "        self.y1_to_x2 = self.y1_to_v1 * (self.v1_to_u1*(self.u1_to_z1*self.z1_to_x2)+ self.v1_to_u3*self.u3_to_z1*self.z1_to_x2)\n",
    "        self.y1_to_x3 = self.y1_to_v1 * (self.v1_to_u3*self.u3_to_z3*self.z3_to_x3) + self.y1_to_v2*self.v2_to_u2*(self.u2_to_x3 +self.u2_to_z2*self.z2_to_x3)\n",
    "\n",
    "        self.y2_to_x1 = self.y2_to_v2 * (self.v2_to_u2 * self.u2_to_z2 * self.z2_to_x1) + self.y2_to_v3* (self.v3_to_u1 *self.u1_to_z1 * self.z1_to_x1 + self.v3_to_u3 *self.u3_to_z1*self.z1_to_x1)\n",
    "        self.y2_to_x2 = self.y2_to_v3 * (self.v3_to_u1 * self.u1_to_z1 *self.z1_to_x2 + self.v3_to_u3*self.u3_to_z1*self.z1_to_x2)\n",
    "        self.y2_to_x3 = self.y2_to_v2 * (self.v2_to_u2 * (self.u2_to_x3+ self.u2_to_z2*self.z2_to_x3))+ self.y2_to_v3*self.v3_to_u3*self.u3_to_z3*self.z3_to_x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 = 24.490479942112778, y2 = 0.6766170068463221\n",
      "1,1,1 输入的雅可比行列式为 \n",
      " 55.76452114990751 29.056029435566057 -13.630322852587943 \n",
      " -9.55244091025101 -4.475992380884129 -1.1360836488405153\n"
     ]
    }
   ],
   "source": [
    "res = Computational_Graph()\n",
    "res.forward(1,1,1)\n",
    "res.backward()\n",
    "\n",
    "print(f\"1,1,1 输入的雅可比行列式为 \\n {res.y1_to_x1} {res.y1_to_x2} {res.y1_to_x3} \\n {res.y2_to_x1} {res.y2_to_x2} {res.y2_to_x3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 = 64.08711522201764, y2 = -6.078667517564845\n",
      "2,1,3 输入的雅可比行列式为 \n",
      " 45.248661012182595 27.320405391278015 -24.266426122991554 \n",
      " 25.74837188319024 0.061386213049383465 23.646691702463592\n"
     ]
    }
   ],
   "source": [
    "res = Computational_Graph()\n",
    "res.forward(2,1,3)\n",
    "res.backward()\n",
    "\n",
    "print(f\"2,1,3 输入的雅可比行列式为 \\n {res.y1_to_x1} {res.y1_to_x2} {res.y1_to_x3} \\n {res.y2_to_x1} {res.y2_to_x2} {res.y2_to_x3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 = 8.170384423581151, y2 = -0.05974457002445096\n",
      "-1,-1,-3 输入的雅可比行列式为 \n",
      " -34.72443898265999 -17.096059449164276 5.658613307843439 \n",
      " -4.530696293583008 -0.37929353394875487 -0.43903810397320575\n"
     ]
    }
   ],
   "source": [
    "res = Computational_Graph()\n",
    "res.forward(-1,-1,-3)\n",
    "res.backward()\n",
    "\n",
    "print(f\"-1,-1,-3 输入的雅可比行列式为 \\n {res.y1_to_x1} {res.y1_to_x2} {res.y1_to_x3} \\n {res.y2_to_x1} {res.y2_to_x2} {res.y2_to_x3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "            x1 = x[0]\n",
    "            x2 = x[1]\n",
    "            x3 = x[2]\n",
    "\n",
    "            z1 = 2*x1 + x2\n",
    "            z2= x1 * 3 * x3   \n",
    "            z3 = -x3\n",
    "\n",
    "            u1 =  torch.sin(z1)\n",
    "            u2 = (2 * x3)+z2\n",
    "            u3 =(2 * z1) + z3\n",
    "\n",
    "            v1 = u1 - u3\n",
    "            v2 = torch.sin(-1*u2)      \n",
    "            v3 = u1 * u3\n",
    "\n",
    "            y1 =  v1**2 + v2**3\n",
    "            y2 =  v2*v3\n",
    "\n",
    "            return y1,y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 55.7645,  29.0560, -13.6303], dtype=torch.float64)\n",
      "tensor([-9.5524, -4.4760, -1.1361], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,1,1], dtype=float, requires_grad=True)\n",
    "model = Net()\n",
    "out_1, out_2 = model(x)\n",
    "\n",
    "out_1.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "x = torch.tensor([1,1,1], dtype=float, requires_grad=True)\n",
    "model = Net()\n",
    "out_1, out_2 = model(x)\n",
    "\n",
    "out_2.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 45.2487,  27.3204, -24.2664], dtype=torch.float64)\n",
      "tensor([25.7484,  0.0614, 23.6467], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2,1,3], dtype=float, requires_grad=True)\n",
    "model = Net()\n",
    "out_1, out_2 = model(x)\n",
    "\n",
    "out_1.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "x = torch.tensor([2,1,3], dtype=float, requires_grad=True)\n",
    "model = Net()\n",
    "out_1, out_2 = model(x)\n",
    "\n",
    "out_2.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-34.7244, -17.0961,   5.6586], dtype=torch.float64)\n",
      "tensor([-4.5307, -0.3793, -0.4390], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1,-1,-3], dtype=float, requires_grad=True)\n",
    "model = Net()\n",
    "out_1, out_2 = model(x)\n",
    "\n",
    "out_1.backward()\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "x = torch.tensor([-1,-1,-3], dtype=float, requires_grad=True)\n",
    "model = Net()\n",
    "out_1, out_2 = model(x)\n",
    "\n",
    "out_2.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务二 numpy实现前向神经网络进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化每一层的W、b矩阵\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    # number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        # 每一层的参数W、b依次编号\n",
    "        # 不使用2个列表装载所有参数，每层参数独立命名\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    print(nn_architecture)\n",
    "    \n",
    "    return params_values\n",
    "\n",
    "# print(init_layers(nn_architecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义激活函数以及他们的导数\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def sigmoid_backward(A, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    dZ = A * sig * (1 - sig)\n",
    "    return dZ\n",
    "\n",
    "def softmax(Z):\n",
    "    Z = np.exp(Z) / np.sum(np.exp(Z), axis = 1, keepdims = True)\n",
    "    return Z\n",
    "\n",
    "def softmax_backward(A, Z):\n",
    "    dZ = np.zeros_like(Z)\n",
    "    m, n = Z.shape\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dZ[i, j] = Z[i, j] * (np.eye(n)[j] - Z[i, :]).dot(A[i, :])\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义前向传播(前一层输出A，本层W、b，激活函数)\n",
    "def single_forward(A, W, b, active_func):\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    return active_func(Z), Z\n",
    "\n",
    "def forward(X, params, nn_architecture):\n",
    "    memory = {} # 记忆字典，用来储存前向传播的结果，供之后的方向传播使用\n",
    "    A_curr = X \n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "\n",
    "        # 开始本层传播\n",
    "        active_func_curr = layer[\"activation\"]\n",
    "        W_curr = params[\"W\" + str(layer_idx)]\n",
    "        b_curr = params[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_forward(A_prev, W_curr, b_curr, active_func_curr)\n",
    "\n",
    "        memory[\"A\"+str(idx)] = A_prev # A_0就是整个网络的输入X\n",
    "        memory[\"Z\"+str(layer_idx)] = Z_curr # 不存在Z_0，Z是某一层的没有经过激活函数的输出\n",
    "\n",
    "    return A_curr, memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播的四个方程式\n",
    "1. 损失函数 对 隐藏层最后非激活输出Z 的偏导 = （损失函数 对 隐藏层最后激活输出a 的偏导） （*） （隐藏层最后激活函数 的导数 = 隐藏层最后激活输出 对 隐藏层最后非激活输出Z 的偏导）\n",
    "2. 损失函数 对 当前层最后非激活输出Z 的偏导 = （当前层权重W * （下一层 本值）） （*） （当前层最后激活函数 的导数）\n",
    "3. 损失函数 对 当前层权重W 的偏导 = （当前层 的2） * （上一层激活输出a）\n",
    "4. 损失函数 对 当前层偏置b 的偏导 = 当前层 的2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数——交叉商损失\n",
    "def cross_entropy_loss(Y_hat, Y):\n",
    "    delta=1e-10  # 避免出现log0\n",
    "    Y_hat = np.clip(Y_hat, delta, 1 - delta) \n",
    "\n",
    "    loss = -np.sum(Y * np.log(Y_hat+delta))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 反向传播\n",
    "# 单层-需要上层A，本层W、Z, 另需要本层A供激活函数求导\n",
    "def single_backward(A_prev, W_curr, Z_curr, dA_curr, active_func_backward):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "\n",
    "    dZ_curr = active_func_backward(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "#  全层\n",
    "def backward(Y_hat, Y, memory, params, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "\n",
    "    dA_prev = - (Y / Y_hat); # 交叉熵损失梯度\n",
    "\n",
    "    for idx, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx = idx + 1\n",
    "        dA_curr = dA_prev\n",
    "\n",
    "        # 开始本层方向传播\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        A_prev = memory[\"A\" + str(idx)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx)]\n",
    "        W_curr = params[\"W\" + str(layer_idx)]\n",
    "        b_curr = params[\"b\" + str(layer_idx)]\n",
    "\n",
    "        active_func_backward = globals()[activ_function_curr.__name__ + '_backward']\n",
    "        dA_prev, dW_curr, db_curr = single_backward(\n",
    "            A_prev, W_curr, Z_curr, dA_curr, active_func_backward)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx)] = db_curr\n",
    "\n",
    "    return grads_values\n",
    "\n",
    "\n",
    "\n",
    "# 参数更新\n",
    "def update(params, grads_values, nn_architecture, learning_rate):\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx =  idx + 1\n",
    "        params[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_dim': 2, 'output_dim': 4, 'activation': <function sigmoid at 0x14edce7a0>}, {'input_dim': 4, 'output_dim': 3, 'activation': <function sigmoid at 0x14edce7a0>}, {'input_dim': 3, 'output_dim': 3, 'activation': <function softmax at 0x14edcea20>}]\n",
      "-0.0019990006661668464\n",
      "epoch: 00000 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00001 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00002 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00003 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00004 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00005 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00006 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00007 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00008 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00009 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00010 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00011 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00012 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00013 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00014 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00015 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00016 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00017 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00018 - cost: -0.00200\n",
      "-0.0019990006661668464\n",
      "epoch: 00019 - cost: -0.00200\n",
      "{'W1': array([[-0.0417839 , -0.00584292],\n",
      "       [-0.21376498,  0.16373635],\n",
      "       [-0.17926158, -0.08401078],\n",
      "       [ 0.0501246 , -0.12485589]]), 'b1': array([[-0.10583126],\n",
      "       [-0.09094922],\n",
      "       [ 0.05517273],\n",
      "       [ 0.22916629]]), 'W2': array([[ 0.00454798, -0.11128188,  0.05415226, -0.0592447 ],\n",
      "       [-0.00216929,  0.11716805, -0.07494733,  0.0006611 ],\n",
      "       [-0.08705712, -0.01466669,  0.02612837, -0.0981678 ]]), 'b2': array([[-0.03297435],\n",
      "       [-0.02420874],\n",
      "       [-0.06202911]]), 'W3': array([[-0.12941478, -0.15347812, -0.02578615],\n",
      "       [-0.03931475,  0.20990898, -0.25563315],\n",
      "       [ 0.01127265,  0.03704445,  0.13596339]]), 'b3': array([[ 2.7711222e-02],\n",
      "       [-1.1059922e-01],\n",
      "       [ 9.7614716e-07]])}\n"
     ]
    }
   ],
   "source": [
    "# 网络基本结构\n",
    "\n",
    "# nn_architecture = [\n",
    "#     {\"input_dim\": 128, \"output_dim\":64, \"activation\": \"sigmoid\"},\n",
    "#     {\"input_dim\": 64, \"output_dim\": 32, \"activation\": \"sigmoid\"},\n",
    "#     {\"input_dim\": 32, \"output_dim\": 9, \"activation\": \"softmax\"},\n",
    "# ]\n",
    "\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\":4, \"activation\": sigmoid},\n",
    "    {\"input_dim\": 4, \"output_dim\": 3, \"activation\": sigmoid},\n",
    "    {\"input_dim\": 3, \"output_dim\": 3, \"activation\": softmax},\n",
    "]\n",
    "\n",
    "\n",
    "X = [3,6]\n",
    "X = np.array(X).reshape(-1, 1)\n",
    "Y = [1,1,0]\n",
    "Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "params = init_layers(nn_architecture, 2)\n",
    "cost_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# print(params)\n",
    "\n",
    "for i in range(epochs):\n",
    "    Y_hat, cashe = forward(X, params, nn_architecture)\n",
    "    \n",
    "    cost = cross_entropy_loss(Y_hat, Y)\n",
    "    print(cost)\n",
    "    cost_history.append(cost)\n",
    "    \n",
    "    grads_values = backward(Y_hat, Y, cashe, params, nn_architecture)\n",
    "    params = update(params, grads_values, nn_architecture, learning_rate)\n",
    "    \n",
    "    print(\"epoch: {:05} - cost: {:.5f}\".format(i, cost))\n",
    "\n",
    "print(params)\n",
    "# print(cost_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
